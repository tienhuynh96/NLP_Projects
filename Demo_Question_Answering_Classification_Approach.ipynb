{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tienhuynh96/NLP_Projects/blob/main/Demo_Question_Answering_Classification_Approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Import libraries and create temp dataset**"
      ],
      "metadata": {
        "id": "5C8jXzW55vTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "4GAgtobPvWRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195098ce-7dfd-4102-b9bf-a4a72f726dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_dataset = [\n",
        "    {\n",
        "        'context': 'My name is AIVN and I am from Vietnam.',\n",
        "        'question': 'What is my name?',\n",
        "        'answer': 'AIVN'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I love painting and my favorite artist is Vicent Van Gogh.',\n",
        "        'question': 'What is my favorite activity?',\n",
        "        'answer': 'painting'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I am studying computer science at the University of Tokyo.',\n",
        "        'question': 'What am I Studying?',\n",
        "        'answer': 'computer science'\n",
        "    },\n",
        "    {\n",
        "        'context': 'My favorite book is \"To kill a Mockingbird\" by Harper Lee.',\n",
        "        'question': 'What is my favorite book?',\n",
        "        'answer': '\"To kill a Mockingbird\"'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I have a pet dog named Max who loves to play fetch',\n",
        "        'question': 'What is the name of my pet?',\n",
        "        'answer': 'Max'\n",
        "    },\n",
        "    {\n",
        "        'context': 'I was born in Paris, but now I live in New york City',\n",
        "        'question': 'Where do I live now?',\n",
        "        'answer': 'New York City'\n",
        "    }\n",
        "    # {\n",
        "    #     'context': '',\n",
        "    #     'question': '',\n",
        "    #     'answer': ''\n",
        "    # },\n",
        "\n",
        "]\n",
        "\n",
        "data_size = len(qa_dataset)\n",
        "data_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQnZjl0g05aD",
        "outputId": "333d0ff4-f570-4927-9d92-646877c7427b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.shape(qa_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cBe6FiG4-h0",
        "outputId": "56dc1df4-a1ee-431f-983d-e27556677cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6,)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Vectorization**"
      ],
      "metadata": {
        "id": "eknboL3K4953"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "text = 'I love AIVN'\n",
        "tokenizer(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLotwcpo05UA",
        "outputId": "77516d81-8468-476e-bd0f-ee1eb25e4c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'aivn']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Create a function to yield list of tokens\n",
        "# This yield function is required in function \"build_vocab_from_iterator\"\n",
        "# Get data from item context and question in data\n",
        "def yield_tokens(data):\n",
        "  for item in data:\n",
        "    yield tokenizer(item['context'] + ' ' + item['question'])\n",
        "\n",
        "# Create vocabulary\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(qa_dataset),\n",
        "    specials = ['<unk>','<pad>','<bos>','<eos>','<sep>']\n",
        ")\n",
        "\n",
        "# Set default index for this vocab is 'unk' = 0, when the unknow word is replace the 'unk'\n",
        "vocab.set_default_index(vocab['<unk>'])\n",
        "# Build vocab (stoi mean string to index)\n",
        "vocab.get_stoi()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Lx7PjY5f05RQ",
        "outputId": "e31b49fb-d121-4519-b773-6510fc46ed3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'to': 24,\n",
              " ',': 25,\n",
              " 'pet': 21,\n",
              " 'who': 61,\n",
              " 'gogh': 39,\n",
              " 'the': 23,\n",
              " 'fetch': 37,\n",
              " 'play': 52,\n",
              " 'van': 56,\n",
              " 'now': 19,\n",
              " 'was': 59,\n",
              " 'a': 14,\n",
              " 'name': 13,\n",
              " 'aivn': 27,\n",
              " 'i': 5,\n",
              " 'studying': 22,\n",
              " 'and': 15,\n",
              " 'where': 60,\n",
              " '<unk>': 0,\n",
              " 'favorite': 11,\n",
              " 'by': 32,\n",
              " 'artist': 28,\n",
              " 'live': 18,\n",
              " '<eos>': 3,\n",
              " 'harper': 40,\n",
              " 'dog': 36,\n",
              " 'loves': 45,\n",
              " '<pad>': 1,\n",
              " 'computer': 34,\n",
              " '.': 10,\n",
              " 'born': 30,\n",
              " 'is': 6,\n",
              " 'my': 7,\n",
              " 'book': 16,\n",
              " 'science': 53,\n",
              " 'of': 20,\n",
              " '<bos>': 2,\n",
              " '<sep>': 4,\n",
              " 'what': 9,\n",
              " 'am': 12,\n",
              " 'named': 48,\n",
              " 'at': 29,\n",
              " 'but': 31,\n",
              " 'in': 17,\n",
              " 'from': 38,\n",
              " 'tokyo': 54,\n",
              " 'city': 33,\n",
              " 'kill': 42,\n",
              " 'lee': 43,\n",
              " 'love': 44,\n",
              " '?': 8,\n",
              " 'do': 35,\n",
              " 'max': 46,\n",
              " 'mockingbird': 47,\n",
              " 'york': 62,\n",
              " 'new': 49,\n",
              " 'painting': 50,\n",
              " 'paris': 51,\n",
              " 'university': 55,\n",
              " 'have': 41,\n",
              " 'vicent': 57,\n",
              " 'activity': 26,\n",
              " 'vietnam': 58}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# Define tokenizer function\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "text = 'I love AIVN'\n",
        "tokens = tokenizer(text)\n",
        "tokens = [vocab[token] for token in tokens]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T_3gVuW8o8N",
        "outputId": "c07d6fce-3dbd-492d-dc08-2e9eb6d9424b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 44, 27]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for answer\n",
        "# Set is for use only unique answer\n",
        "classes = set([item['answer'] for item in qa_dataset])\n",
        "# Class to index\n",
        "classes_to_idx = {\n",
        "    cls_name : idx for idx, cls_name in enumerate(classes)\n",
        "}\n",
        "# Index to class\n",
        "idx_to_classes = {\n",
        "    idx : cls_name for idx, cls_name in enumerate(classes)\n",
        "}\n",
        "\n",
        "print(idx_to_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "annesNMP8o4x",
        "outputId": "c5eebab1-3b35-4298-bd8d-a7f0a4dccc0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'AIVN', 1: '\"To kill a Mockingbird\"', 2: 'New York City', 3: 'Max', 4: 'painting', 5: 'computer science'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad an truncate\n",
        "max_seq_len = 10\n",
        "PAD_IDX = vocab['<pad>']\n",
        "# PAD_IDX = 1\n",
        "\n",
        "# define pad and truncate function\n",
        "def pad_and_truncate(input_ids, max_seq_len):\n",
        "  if len(input_ids) > max_seq_len:\n",
        "    input_ids = input_ids[:max_seq_len]\n",
        "  elif len(input_ids) < max_seq_len:\n",
        "    input_ids += [PAD_IDX] * (max_seq_len - len(input_ids))\n",
        "\n",
        "  return input_ids"
      ],
      "metadata": {
        "id": "Im8xAlhH05Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "# Pad an truncate\n",
        "max_seq_len = 10\n",
        "PAD_IDX = 1\n",
        "\n",
        "text = 'I love AIVN'\n",
        "tokens = tokenizer(text)\n",
        "tokens = [vocab[token] for token in tokens]\n",
        "print(tokens)\n",
        "padded_tokens = pad_and_truncate(tokens, max_seq_len)\n",
        "print(padded_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLR0YIn4Aah_",
        "outputId": "72124a23-13e7-4047-e76b-9bbdc0a502ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 44, 27]\n",
            "[5, 44, 27, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vectorize function\n",
        "def vectorize(question, context):\n",
        "  input_question_ids = [vocab[token] for token in tokenizer(question)]\n",
        "  input_context_ids = [vocab[token] for token in tokenizer(context)]\n",
        "\n",
        "  input_question_ids = pad_and_truncate(input_question_ids, MAX_QUESTION_LEN)\n",
        "  input_context_ids = pad_and_truncate(input_context_ids, MAX_CONTEXT_LEN)\n",
        "\n",
        "  input_question_ids = torch.tensor(input_question_ids, dtype = torch.long)\n",
        "  input_context_ids = torch.tensor(input_context_ids, dtype = torch.long)\n",
        "\n",
        "  return input_question_ids, input_context_ids\n",
        "\n"
      ],
      "metadata": {
        "id": "1FbItdwuAae_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_QUESTION_LEN, MAX_CONTEXT_LEN = 10, 10\n",
        "input_question_ids, input_context_ids = vectorize(\n",
        "    qa_dataset[0]['question'],\n",
        "    qa_dataset[0]['context']\n",
        ")\n",
        "\n",
        "print(input_question_ids)\n",
        "print(input_context_ids)\n",
        "print(classes_to_idx[qa_dataset[0]['answer']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9vAHd1ZzG7H",
        "outputId": "b769b6be-b506-4f5b-8862-4a596b0275af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 9,  6,  7, 13,  8,  1,  1,  1,  1,  1])\n",
            "tensor([ 7, 13,  6, 27, 15,  5, 12, 38, 58, 10])\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Create datasets**"
      ],
      "metadata": {
        "id": "gehgPajV0bkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QADataset(Dataset):\n",
        "  def __init__(self,data):\n",
        "    self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    item = self.data[idx]\n",
        "    question_text = item['question']\n",
        "    context_text = item['context']\n",
        "    answer_text = item['answer']\n",
        "\n",
        "    input_question_ids, input_context_ids = vectorize(question_text, context_text)\n",
        "\n",
        "    answer_id = classes_to_idx[answer_text]\n",
        "    answer_id = torch.tensor(answer_id, dtype = torch.long)\n",
        "\n",
        "    return  input_question_ids, input_context_ids, answer_id\n"
      ],
      "metadata": {
        "id": "OSfGuItGAacY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode\n",
        "# Define decode function: convert id to token\n",
        "def decode(input_ids):\n",
        "  return ' '.join([vocab.lookup_token(token) for token in input_ids])"
      ],
      "metadata": {
        "id": "orMbBrZW05L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = QADataset(qa_dataset)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 2, shuffle=True)"
      ],
      "metadata": {
        "id": "YIvsl5cV2ofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOrkC6mf2oaY",
        "outputId": "8d109b23-686a-43a1-81b5-56c6bbd36a65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 9,  6,  7, 11, 26,  8,  1,  1,  1,  1],\n",
              "         [ 9, 12,  5, 22,  8,  1,  1,  1,  1,  1]]),\n",
              " tensor([[ 5, 44, 50, 15,  7, 11, 28,  6, 57, 56],\n",
              "         [ 5, 12, 22, 34, 53, 29, 23, 55, 20, 54]]),\n",
              " tensor([4, 5])]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "  input_question_ids, input_context_ids, answer_id = batch\n",
        "  print(input_question_ids, input_context_ids, answer_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM-Ed_Tm2oW9",
        "outputId": "bf3c1155-5e86-448f-e050-a89cc88b05e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 9,  6,  7, 11, 16,  8,  1,  1,  1,  1],\n",
            "        [ 9, 12,  5, 22,  8,  1,  1,  1,  1,  1]]) tensor([[ 7, 11, 16,  6, 24, 42, 14, 47, 32, 40],\n",
            "        [ 5, 12, 22, 34, 53, 29, 23, 55, 20, 54]]) tensor([1, 5])\n",
            "tensor([[ 9,  6,  7, 13,  8,  1,  1,  1,  1,  1],\n",
            "        [60, 35,  5, 18, 19,  8,  1,  1,  1,  1]]) tensor([[ 7, 13,  6, 27, 15,  5, 12, 38, 58, 10],\n",
            "        [ 5, 59, 30, 17, 51, 25, 31, 19,  5, 18]]) tensor([0, 2])\n",
            "tensor([[ 9,  6,  7, 11, 26,  8,  1,  1,  1,  1],\n",
            "        [ 9,  6, 23, 13, 20,  7, 21,  8,  1,  1]]) tensor([[ 5, 44, 50, 15,  7, 11, 28,  6, 57, 56],\n",
            "        [ 5, 41, 14, 21, 36, 48, 46, 61, 45, 24]]) tensor([4, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Create models**"
      ],
      "metadata": {
        "id": "BZbi1dL43n-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This model use Bi-LSTM\n",
        "class QAModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_classes ):\n",
        "    super(QAModel, self).__init__()\n",
        "    # Get embedding for question and context\n",
        "    self.question_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.context_embedding  = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # Use bi-LSTM\n",
        "    # concat question embed, context embed\n",
        "    self.lstm = nn.LSTM(\n",
        "        embedding_dim, hidden_size,\n",
        "        num_layers    = n_layers,\n",
        "        batch_first   = True,\n",
        "        bidirectional = True\n",
        "    )\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size * 2, n_classes)\n",
        "\n",
        "  def forward(self, question, context):\n",
        "    question_embed = self.question_embedding(question)\n",
        "    context_embed = self.context_embedding(context)\n",
        "    # print(question_embed[0].shape, context_embed[0].shape)\n",
        "\n",
        "    combined = torch.cat (\n",
        "        (question_embed, context_embed),\n",
        "        dim = 1\n",
        "    )\n",
        "    # print(combined[0].shape)\n",
        "\n",
        "    lstm_out, _ = self.lstm(combined)\n",
        "    # print(lstm_out.shape)\n",
        "\n",
        "    lstm_out = lstm_out[:,-1, :]  # get the last hidden state (output: bs, seq_len, hidden_units)\n",
        "    # print(lstm_out.shape)\n",
        "\n",
        "    out = self.fc(lstm_out)\n",
        "    # print(out.shape)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CkuZ4FP2oS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_SIZE = 128\n",
        "VOCAB_SIZE = len(vocab)\n",
        "N_LAYERS = 2\n",
        "N_CLASSES = len(classes)\n",
        "\n",
        "model = QAModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, N_CLASSES)\n",
        "\n",
        "input_context = torch.randint(0, 10, size=(1, MAX_CONTEXT_LEN))\n",
        "input_question = torch.randint(0, 10, size=(1, MAX_QUESTION_LEN))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  logits = model(input_question, input_context)\n",
        "\n",
        "print(logits.shape)"
      ],
      "metadata": {
        "id": "02I12tR_A5Fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45595ca-3e82-460d-8a1a-997e91ae6c9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check lai doan LSTM, embedding dim vs hidden size"
      ],
      "metadata": {
        "id": "3vkwbmFoA498"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Training models**"
      ],
      "metadata": {
        "id": "-kt0WOaMATJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-3\n",
        "EPOCHS = 20\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for _ in range(EPOCHS):\n",
        "  for idx, (input_question_ids, input_context_ids, answer_id) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_question_ids, input_context_ids)\n",
        "    loss = criterion(outputs, answer_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfuG8ILK6ta2",
        "outputId": "9bbf10d3-5b20-41b8-96aa-d711631a1aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.8037892580032349\n",
            "1.8395943641662598\n",
            "1.7732521295547485\n",
            "1.6463603973388672\n",
            "1.7430801391601562\n",
            "1.6934337615966797\n",
            "1.633399486541748\n",
            "1.670586109161377\n",
            "1.490000605583191\n",
            "1.4819023609161377\n",
            "1.5229260921478271\n",
            "1.4440586566925049\n",
            "1.2778379917144775\n",
            "1.3840556144714355\n",
            "1.3430676460266113\n",
            "1.093340516090393\n",
            "1.230642318725586\n",
            "1.090537428855896\n",
            "1.0080044269561768\n",
            "1.0771441459655762\n",
            "0.6168181896209717\n",
            "0.8835021257400513\n",
            "0.4259416162967682\n",
            "0.6235527992248535\n",
            "0.6059258580207825\n",
            "0.45278531312942505\n",
            "0.19909530878067017\n",
            "0.2641212046146393\n",
            "0.2421870231628418\n",
            "0.19094818830490112\n",
            "0.1267157942056656\n",
            "0.138309508562088\n",
            "0.08771412819623947\n",
            "0.052369602024555206\n",
            "0.08635278046131134\n",
            "0.03566078469157219\n",
            "0.02458029054105282\n",
            "0.02104884944856167\n",
            "0.03563749045133591\n",
            "0.015992309898138046\n",
            "0.022911496460437775\n",
            "0.011570308357477188\n",
            "0.01159745454788208\n",
            "0.00997006893157959\n",
            "0.008980734273791313\n",
            "0.00824988167732954\n",
            "0.006879919208586216\n",
            "0.006042861379683018\n",
            "0.005948035977780819\n",
            "0.005709173157811165\n",
            "0.0038714581169188023\n",
            "0.0046789082698524\n",
            "0.0033633923158049583\n",
            "0.004169392865151167\n",
            "0.003875593189150095\n",
            "0.003059632144868374\n",
            "0.0029681907035410404\n",
            "0.0027989866212010384\n",
            "0.002620900748297572\n",
            "0.002906915731728077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Test**"
      ],
      "metadata": {
        "id": "cCuwXzI5BdVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  sample = qa_dataset[1]\n",
        "  context, question, answer = sample.values()\n",
        "  question_ids, context_ids = vectorize(question, context)\n",
        "  question_ids = question_ids.unsqueeze(0)\n",
        "  context_ids = context_ids.unsqueeze(0)\n",
        "\n",
        "  outputs = model(question_ids, context_ids)\n",
        "  _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "  print(f'Context: {context}')\n",
        "  print(f'Question: {question}')\n",
        "  print(f'Prediction: {idx_to_classes[predicted.numpy()[0]]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yDGCdVP6tYY",
        "outputId": "288a20bb-c5ef-444a-b563-15be3f391068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: I love painting and my favorite artist is Vicent Van Gogh.\n",
            "Question: What is my favorite activity?\n",
            "Prediction: painting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D4XggfVd6tVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rHRAhmqO6tTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "392EEkxe6tQe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}